{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOimLzlovSpxlBAGJ/vN++z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snavasg/NLP_Analysis/blob/main/Excercise1_code_Navas_Gomez.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1: Data pre-processing"
      ],
      "metadata": {
        "id": "GPFcvlIbrhHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries and Downloads"
      ],
      "metadata": {
        "id": "aA-Nn2lJr8Dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries\n",
        "# Import necessary libraries for the code\n",
        "import csv\n",
        "import pandas as pd\n",
        "from googletrans import Translator  # Version used to install googletrans==4.0.0-rc1 (pip install googletrans==4.0.0-rc1)\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Download\n",
        "# Download the list of stop words in English if you haven't already.\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Download the Punkt tokenizer if you haven't already.\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "id": "eUzo1CK1r5H3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f681204-2baa-4492-9817-38f21a2d6c7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Future Functions Definitions\n"
      ],
      "metadata": {
        "id": "KzQmLrBesyKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###### Stop Words #######\n",
        "# Function to remove common stop words from a given text.\n",
        "def eliminar_stop_words(texto):\n",
        "    stop_words = set(stopwords.words('english'))# Get a set of common stop words in English\n",
        "    palabras = texto.split()   # Split the input text into words\n",
        "    palabras_filtradas = [palabra for palabra in palabras if palabra.lower() not in stop_words]# Filter out words that are in the stop words set\n",
        "    texto_filtrado = ' '.join(palabras_filtradas)  # Reconstruct the text without stop words\n",
        "    return texto_filtrado\n",
        "\n",
        "###### Special Characters #######\n",
        "# Function to clean the text by removing special characters and punctuation.\n",
        "def limpiar_texto(texto):\n",
        "    texto_limpio = ''.join([caracter for caracter in texto if caracter not in string.punctuation]) # Remove special characters and punctuation from the text\n",
        "    return texto_limpio\n",
        "\n",
        "###### Tokenization ########\n",
        "# Function to tokenize the input text into individual words.\n",
        "def tokenizar_texto(texto):\n",
        "    tokens = nltk.word_tokenize(texto)  # Tokenize the input text using the NLTK word tokenizer\n",
        "    return tokens\n",
        "\n",
        "###### Vectorization ########\n",
        "# Function to vectorize the given text data using Word2Vec.\n",
        "# Parameters:\n",
        "#   - texto_column: A list of tokenized sentences or text data. This input data will be used to train the Word2Vec model.\n",
        "\n",
        "#   - vector_size: The dimensionality of the word vectors. This parameter determines the length of the word vectors and,\n",
        "#     consequently, the amount of information they can capture. It's typically set to a value between 100 and 300, where higher\n",
        "#     values can capture more information but may require more data.\n",
        "\n",
        "############\n",
        "# NOTE: I use vector_size of 6 because the visualization of vectors in the output .csv is very complicated\n",
        "############\n",
        "\n",
        "#   - window: The maximum distance between a sentence's current and predicted word. It defines the context window size for\n",
        "#     the model. Words outside this window are not considered for context. A smaller value, such as 5, captures the local context,\n",
        "#     while a larger value, such as 10, captures the broader context.\n",
        "\n",
        "#   - min_count: Ignores all words with a total frequency lower than this value. Setting it to 1 means that even infrequent\n",
        "#     words are included in the model, while higher values filter out rare words. This parameter helps control the vocabulary size.\n",
        "\n",
        "#   - sg: Training algorithm for Word2Vec. Use 0 for Continuous Bag of Words (CBOW) and 1 for Skip-gram. CBOW predicts the\n",
        "#     target word based on its context words, while Skip-gram predicts context words given the target word. The choice between\n",
        "#     CBOW and Skip-gram affects the focus of the word vectors.\n",
        "\n",
        "def vectorizar_texto(texto_column, vector_size=6, window=5, min_count=1, sg=0):\n",
        "    # Train a Word2Vec model with the specified parameters\n",
        "    model = Word2Vec(texto_column, vector_size=vector_size, window=window, min_count=min_count, sg=sg)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "lmYJ3Ea4swrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "0GP0W2dsr--Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGdGIkjyLo5-"
      },
      "outputs": [],
      "source": [
        "# Open the CSV file with 'latin-1' encoding (ISO-8859-1)\n",
        "with open('/content/File1.csv', mode='r', encoding='latin-1') as file:\n",
        "    # Configure the CSV reader with delimiter (comma) and quote character (double quotes)\n",
        "    reader = csv.reader(file, delimiter=',', quotechar='\"')\n",
        "    # Create a DataFrame from the CSV data and set column names\n",
        "    df = pd.DataFrame(reader, columns=next(reader))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Pre-processing\n"
      ],
      "metadata": {
        "id": "JRBMTAmlsINH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all text data in the DataFrame to lowercase while preserving non-string values\n",
        "df = df.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
        "# Create a single instance of the translator\n",
        "translator = Translator()\n",
        "# Translate the elements in the 'INDICATOR_NAME_RAW' column sequentially to English\n",
        "df['INDICATOR_NAME_RAW_ENGLISH'] = df['INDICATOR_NAME_RAW'].apply(lambda text: translator.translate(text, dest='en').text.lower())"
      ],
      "metadata": {
        "id": "UMQakyJ0sPyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FIRST, REMOVE STOP WORDS BECAUSE IF WE EXAMINE 'set(stopwords.words('english'))', it includes options like \"weren't,\" \"shouldn't,\" \"aren't,\" etc.\n",
        "# These are contractions of words like \"were not,\" \"should not,\" \"are not,\" which also contain the special character \"'\", so if we first\n",
        "# remove special characters, we would get \"werent,\" \"shouldnt,\" \"arent,\" and then we wouldn't be able to identify them as stop words, but they are.\n",
        "\n",
        "# Apply the function to remove stop words to the 'INDICATOR_NAME_RAW_ENGLISH' column\n",
        "df['INDICATOR_NAME_RAW_ENGLISH_CLEAN'] = df['INDICATOR_NAME_RAW_ENGLISH'].apply(eliminar_stop_words)\n",
        "# Apply the cleaning function to the 'INDICATOR_NAME_RAW_ENGLISH_CLEAN' column\n",
        "df['INDICATOR_NAME_RAW_ENGLISH_CLEAN'] = df['INDICATOR_NAME_RAW_ENGLISH_CLEAN'].apply(limpiar_texto)\n",
        "# Apply the tokenization function to the 'INDICATOR_NAME_RAW_ENGLISH_CLEAN' column\n",
        "df['TOKENS'] = df['INDICATOR_NAME_RAW_ENGLISH_CLEAN'].apply(tokenizar_texto)\n",
        "# Call the function to vectorize the text\n",
        "df['MODEL_EMBEDDINGS'] = vectorizar_texto(df['TOKENS']).wv\n",
        "# EMBEDDINGS_LIST column represents the values of the vectorization, in the form of a list, since\n",
        "# the MODEL_EMBEDDINGS column cannot be displayed when exporting, so it is created only for contextual purposes.\n",
        "df['EMBEDDINGS_LIST'] = df['MODEL_EMBEDDINGS'].apply(lambda x: x[0] if x else [])\n",
        "\n"
      ],
      "metadata": {
        "id": "dHWES-vXaPFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"Excercise1_output_Navas-Gomez.csv\", index=False)  # I suggest when load the Excercise1_output_Navas-Gomez.csv the following python code"
      ],
      "metadata": {
        "id": "PYvxrGTX_zYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('/content/Excercise1_output_Navas-Gomez.csv', mode='r', encoding='latin-1') as file:\n",
        "#     # Configure the CSV reader with delimiter (comma) and quote character (double quotes)\n",
        "#     reader = csv.reader(file, delimiter=',', quotechar='\"')\n",
        "#     # Create a DataFrame from the CSV data and set column names\n",
        "#     df = pd.DataFrame(reader, columns=next(reader))"
      ],
      "metadata": {
        "id": "ljLTvCrGHEZ8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}